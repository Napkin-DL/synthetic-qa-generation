{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate QnA synthetic dataset from a Complex PDF using Unstructured\n",
    "\n",
    "### Overview\n",
    "We process the PDF by dividing it into three parts.\n",
    "\n",
    "- **Text-heavy** - Text-heavy PDF can be processed with open source without the need to use toolkits like Azure AI Document Intelligence or Unstructured.\n",
    "- **Image-heavy** - Image-heavy PDF can be converted the entire page to images and let a multimodal LLM like GPT-4o summarize each page.\n",
    "- **Mixed** - After reading the document with Azure AI Document Intelligence, we replace the image descriptions inside the figure tags with text summarized by a multimodal LLM. (Often the image descriptions are blank or have only a short caption.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Read & Preprocess PDF file\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the PDFs into individual pages\n",
    "Only use a poration of the PDF documents for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, shutil, random\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from unstructured.cleaners.core import clean_bullets, clean_extra_whitespace, remove_punctuation\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader, UnstructuredMarkdownLoader, UnstructuredAPIFileLoader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader, UnstructuredCSVLoader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "raw_data_dir = \"raw_data\"\n",
    "splitted_raw_data_dir = f\"splitted_{raw_data_dir}\"\n",
    "\n",
    "#file_path = f\"{raw_data_dir}/prod-unst-pdf/[Sales Talk] 3. QnA3_Handling Objection_(S24)_240227.pdf\"\n",
    "file_path = f\"{raw_data_dir}/prod-unst-pdf/SM-S92X_UG_UU_Kor_Rev.1.1_240129.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# Open the first PDF document\n",
    "doc1 = fitz.open(file_path)\n",
    "split_pages = [(1, 15)]\n",
    "\n",
    "for idx, s in enumerate(split_pages):\n",
    "    # Create a new empty PDF document\n",
    "    doc2 = fitz.open()\n",
    "\n",
    "    # Insert the first 2 pages of doc1 into doc2\n",
    "    doc2.insert_pdf(doc1, from_page=s[0], to_page=s[1])\n",
    "\n",
    "    # Save the modified document\n",
    "    doc2.save(f\"{raw_data_dir}/prod-unst-pdf/s24-user-manual-part{idx}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'splitted_raw_data' and its contents have been deleted.\n",
      "### PDF Content Analysis Result:\n",
      "Text pages: [0, 1, 8, 10, 12, 13]\n",
      "Mixed pages: [2, 3, 4, 5, 6, 7, 11, 14]\n",
      "Image pages: [9]\n"
     ]
    }
   ],
   "source": [
    "from util.common_utils import delete_folder_and_make_folder\n",
    "from util.preprocess import remove_short_sentences, remove_small_images, analyze_pdf_page_content, split_pdf\n",
    "\n",
    "file_path = f\"{raw_data_dir}/prod-unst-pdf/s24-user-manual-part0.pdf\"\n",
    "result = analyze_pdf_page_content(file_path)\n",
    "delete_folder_and_make_folder(splitted_raw_data_dir)    \n",
    "\n",
    "print(\"### PDF Content Analysis Result:\")\n",
    "for content_type, pages in result.items():\n",
    "    print(f\"{content_type} pages: {pages}\")\n",
    "    split_pdf(file_path, f\"{splitted_raw_data_dir}/{content_type}.pdf\", pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Mixed page (Images and text mixed appropriately)\n",
    "After reading the document with UnstructuredFileLoader, we replace the image descriptions inside the figure tags with text summarized by a multimodal LLM. (Often the image descriptions are blank or have only a short caption.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.1 s, sys: 912 ms, total: 49 s\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pdf_mixed_path = f\"{splitted_raw_data_dir}/Mixed.pdf\"\n",
    "\n",
    "chunk_size = 1500\n",
    "new_after_n_chars = 1200\n",
    "combine_text_under_n_chars = 1000\n",
    "chunk_overlap = 100\n",
    "max_tokens = 1024\n",
    "image_dir = \"./images\"\n",
    "\n",
    "loader = UnstructuredFileLoader(\n",
    "    file_path=pdf_mixed_path,\n",
    "\n",
    "    chunking_strategy = \"by_title\",\n",
    "    mode=\"elements\",\n",
    "\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    hi_res_model_name=\"yolox_quantized\", #\"detectron2_onnx\", \"yolox\", \"yolox_quantized\"\n",
    "\n",
    "    extract_images_in_pdf=True,\n",
    "    #skip_infer_table_types='[]', # ['pdf', 'jpg', 'png', 'xls', 'xlsx', 'heic']\n",
    "    skip_infer_table_types=True, ## enable to get table as html using tabletrasformer\n",
    "\n",
    "    extract_image_block_output_dir=image_dir,\n",
    "    extract_image_block_to_payload=False, ## False: to save image\n",
    "\n",
    "    max_characters=chunk_size,\n",
    "    new_after_n_chars=new_after_n_chars,\n",
    "    combine_text_under_n_chars=combine_text_under_n_chars, # 이 문자 수 이하의 텍스트는 결합\n",
    "\n",
    "    languages= [\"kor+eng\"],\n",
    "\n",
    "    post_processors=[clean_bullets, clean_extra_whitespace, remove_punctuation]\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # texts: 2 \n",
      " # tables: 0 \n",
      " # images: 13\n"
     ]
    }
   ],
   "source": [
    "images = remove_small_images(image_dir, image_dim_thres=16)\n",
    "tables, texts = [], []\n",
    "\n",
    "for doc in docs:\n",
    "    category = doc.metadata[\"category\"]\n",
    "    if category == \"Table\": tables.append(doc)\n",
    "    else: texts.append(doc)\n",
    "\n",
    "print (f' # texts: {len(texts)} \\n # tables: {len(tables)} \\n # images: {len(images)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=max_tokens,\n",
    "    openai_api_version=\"2024-05-01-preview\",\n",
    "    azure_deployment=\"gpt-4o\"                       \n",
    ")\n",
    "\n",
    "system_prompt = \"You are an assistant tasked with describing table or image, specialized in Smartphone product.\"\n",
    "system_message_template = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "human_prompt = [\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": \"data:image/png;base64,\" + \"{image_base64}\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": '''Given image, give a concise summary in Korean. Don't insert any XML tag such as <text> and </text> when answering.'''\n",
    "    },\n",
    "]\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message_template,\n",
    "        human_message_template\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = prompt | llm | StrOutputParser()\n",
    "#summarize_chain = {\"image_base64\": lambda x:x} | prompt | llm_text | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 143 ms, sys: 42.7 ms, total: 185 ms\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from util.preprocess import encode_image_base64\n",
    "#images = glob(os.path.join(image_path, \"*.jpg\"))\n",
    "base64_images = [encode_image_base64(img_path) for img_path in images]\n",
    "image_summaries = summarize_chain.batch(base64_images, {\"max_concurrency\": 3})\n",
    "image_summaries = remove_short_sentences(image_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of splits (mixed case): 14\n"
     ]
    }
   ],
   "source": [
    "from util.preprocess import split_text_using_tiktoken\n",
    "\n",
    "texts_tiktoken = split_text_using_tiktoken(texts, chunk_size, chunk_overlap)\n",
    "\n",
    "mixed_chunks = image_summaries + texts_tiktoken\n",
    "print(\"Length of splits (mixed case): \" + str(len(mixed_chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Text-heavy\n",
    "Text-heavy PDFs can be processed with open source without the need to use toolkits like Azure AI Document Intelligence or Unstructured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n",
      "page_content='2\\n시작하기\\n4\\t\\n각 부분의 이름과 역할\\n11\\t\\n배터리 충전하기\\n15\\t\\nNano-SIM 카드 및 eSIM\\n19\\t\\n전원 켜기/끄기\\n20\\t\\n제품 초기 설정\\n20\\t\\n삼성 계정\\n20\\t\\n이전 기기의 데이터 가져오기(Smart Switch)\\n22\\t\\n화면 알아두기\\n30\\t\\n알림창\\n31\\t\\n화면 캡처 및 화면 녹화\\n33\\t\\n문자 입력\\n35\\t\\n텍스트 추출\\n앱과 기능 알아보기\\n36\\t\\n앱 설치 및 관리\\n37\\t\\nS펜(Galaxy S24 Ultra)\\n50\\t\\n전화\\n54\\t\\n연락처\\n56\\t\\n메시지\\n58\\t\\n카메라\\n74\\t\\n갤러리\\n79\\t\\nAR 존\\n84\\t\\n빅스비\\n85\\t\\n빅스비 비전\\n86\\t\\n멀티윈도우(여러 앱 동시에 사용하기)\\n89\\t\\n삼성 인터넷\\n90\\t\\nSamsung Pay(삼성 페이)\\n차례\\n93\\t\\n삼성 헬스\\n94\\t\\nSamsung Notes(삼성 노트)\\n98\\t\\nSamsung Members(삼성 멤버스)\\n99\\t\\nSamsung Kids(삼성 키즈)\\n99\\t\\nSamsung Global Goals(삼성 글로벌 골)\\n99\\t\\nSamsung Find(삼성 파인드)\\n100\\t 삼성닷컴\\n100\\t Galaxy Wearable(갤럭시 웨어러블)\\n100\\t PENUP(펜업)(Galaxy S24 Ultra)\\n100\\t 캘린더\\n101\\t\\n리마인더(할 일 알림 받기)\\n102\\t 라디오\\n103\\t 음성 녹음\\n104\\t 내 파일(파일 확인 및 관리하기)\\n104\\t 시계\\n105\\t 계산기\\n105\\t Gaming Hub(게이밍 허브)\\n106\\t 게임 부스터(게임 환경 설정하기)\\n107\\t\\nSmartThings(스마트싱스)\\n108\\t 콘텐츠(파일) 공유하기\\n109\\t Music Share(뮤직 쉐어)\\n110\\t\\nSmart View(스마트 뷰)(TV 화면으로 보기)\\n111\\t\\nWindows와 연결(컴퓨터와 연결해 사용하기)\\n112\\t\\nSamsung DeX(삼성 덱스)\\n116\\t\\nGoogle(구글) 앱\\n117\\t\\n이동통신 사업자 앱' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 0, 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Chunk 1\n",
      "page_content='차례\\n3\\n설정 알아보기\\n121\\t\\n설정 소개\\n121\\t\\n삼성 계정\\n121\\t\\n연결\\n122\\t\\nWi-Fi\\n124\\t\\n블루투스\\n125\\t\\nNFC 및 비접촉 결제\\n126\\t\\n네트워크 상태 표시\\n126\\t\\n데이터 절약 모드\\n126\\t\\n앱별 네트워크 설정\\n127\\t\\n모바일 핫스팟\\n127\\t\\n기타 연결 설정\\n129\\t\\n기기 간 연결\\n130\\t 모드 및 루틴\\n130\\t 모드 사용하기\\n130\\t 루틴 사용하기\\n131\\t\\n소리 및 진동\\n132\\t\\n음질 및 음향 효과\\n132\\t\\n앱 소리 분리 재생\\n132\\t\\n알림\\n133\\t\\n디스플레이\\n134\\t\\n부드러운 모션 및 화면 전환\\n134\\t\\n화면 모드 변경 및 색상 조절\\n135\\t\\n배터리\\n136\\t\\n배경화면 및 스타일\\n136\\t\\n테마\\n136\\t\\n홈 화면\\n136\\t\\n잠금화면 및 AOD\\n137\\t\\n잠금 해제 유지\\n137\\t\\nAlways On Display(올웨이즈 온 \\n디스플레이)(꺼진 화면에 정보 표시하기)\\n138\\t 보안 및 개인정보 보호\\n139\\t\\n얼굴 인식\\n140\\t 지문 인식\\n142\\t\\n보안 폴더\\n144\\t 보안 Wi-Fi\\n145\\t\\nSamsung Pass(삼성 패스)\\n148\\t 위치\\n148\\t 안전 및 긴급\\n149\\t\\n계정 및 백업\\n149\\t\\n삼성 클라우드\\n150\\t Google(구글)\\n150\\t 유용한 기능\\n151\\t\\n향상된 인텔리전스\\n152\\t\\n모션 및 제스처\\n152\\t\\n영상통화 효과\\n153\\t\\n듀얼 메신저\\n153\\t\\n디지털 웰빙 및 자녀 보호 기능\\n154\\t\\n디바이스 케어\\n154\\t\\n애플리케이션\\n155\\t\\n일반\\n156\\t\\n접근성\\n156\\t\\n소프트웨어 업데이트\\n156\\t\\n휴대전화 정보\\n157\\t\\n이동통신 사업자 설정\\n제품 사용 시 알아두기\\n160\\t 제품 사용 시 주의 사항\\n162\\t\\n구성품 및 별매품 안내 사항\\n163\\t\\n제품 방수 및 방진 기능 주의 사항\\n164\\t\\n제품에서 열이 발생하는 경우 및 조치 방법\\n167\\t\\n개인정보 및 제품 분실 시 피해 방지 설정\\n부록\\n169\\t\\n접근성\\n181\\t\\n서비스를 요청하기 전에 확인할 사항\\n191\\t\\n규격 및 특성' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 1, 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Chunk 2\n",
      "page_content='시작하기\\n10\\n\\t\\n•\\n미디어 파일 재생, 스피커폰 통화 등 스피커를 사용하는 경우에는 제품을 귀에 가까이 대지 마세요.\\n\\t\\n•\\n직사광선 등 강한 광원에 카메라 렌즈가 노출되지 않도록 주의하세요. 직사광선과 같은 강한 광원에 \\n카메라 렌즈가 노출되면 카메라 이미지 센서가 손상될 수 있습니다. 손상된 이미지 센서는 회복되지 \\n않으며 그로 인해 촬영 시 점이나 얼룩이 생길 수 있습니다.\\n\\t\\n•\\n제품의 유리 또는 아크릴이 깨지거나 파손된 상태로 사용할 경우 다칠 위험이 있으니 반드시 삼성전자 \\n서비스 센터에서 수리 후 사용하세요.\\n\\t\\n•\\n마이크, 스피커 또는 수화부에 먼지나 이물질이 들어가거나 해당 부분이 가려지면 제품의 소리가 \\n작아지거나 특정 기능이 정상적으로 동작하지 않을 수 있습니다. 이때, 뾰족한 물체 등으로 먼지나 \\n이물질을 무리해서 제거하는 경우 제품이 손상될 수 있으니 주의하세요.\\n\\t\\n•\\n화면에는 지문 방지 코팅이 적용되어 있습니다. 이 코팅은 일반적으로 시간이 경과함에 따라 약화되며, \\n지속적인 압력이나 마찰이 가해질 경우 코팅 효과가 감소되거나 코팅 벗겨짐 등 손상이 발생될 수 \\n있습니다.\\n\\t\\n•\\n다음과 같은 경우에는 통화 품질 및 수신율이 떨어지거나, 배터리가 많이 소모될 수 있습니다.\\n\\t\\n－안테나 부분에 금속 재질의 스티커를 붙이는 경우\\n\\t\\n－금속 재질의 커버를 사용하는 경우\\n\\t\\n－통화나 모바일 데이터와 같은 제품의 기능 이용 중에 안테나 부분을 만질 경우\\n\\t\\n•\\n에어 벤트 홈이 보호 필름이나 스티커 등으로 가려져 있으면 통화 중이나 미디어 재생 시 잡음이 발생할 \\n수 있습니다.\\n\\t\\n•\\n근접/조도 센서 및 그 주변을 스티커나 커버 등의 액세서리로 가리지 마세요. 센서가 올바르게 \\n동작하지 않을 수 있습니다.\\n\\t\\n•\\n근접 센서의 동작으로 통화 시에는 제품 위쪽 부분에 불빛이 깜박일 수 있습니다.\\n하드웨어 버튼\\n버튼\\n기능\\n측면 버튼\\n\\t\\n•\\n제품이 꺼진 상태에서 길게 누르면 전원이 켜집니다.\\n\\t\\n•\\n짧게 누르면 화면이 켜지거나 잠깁니다.\\n\\t\\n•\\n길게 누르면 빅스비와 대화를 시작할 수 있습니다. 자세한 내용은 빅스비 \\n사용하기를 참고하세요.\\n\\t\\n•\\n두 번 누르거나 길게 누르면 설정해 놓은 앱 또는 기능을 실행할 수 \\n있습니다.\\n측면 버튼 + 음량(하) 버튼\\n\\t\\n•\\n동시에 짧게 누르면 화면을 캡처할 수 있습니다.\\n\\t\\n•\\n동시에 길게 누르면 전원을 끌 수 있습니다.' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 2, 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Length of splits (text-heay case): 7\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.pdf import PyMuPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_text_path = f\"{splitted_raw_data_dir}/Text.pdf\"\n",
    "loader = PyMuPDFLoader(pdf_text_path)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "for idx, chunk in enumerate(text_chunks):\n",
    "    print(f\"Chunk {idx}\\n{chunk}\")\n",
    "    print(\"=\"*80)\n",
    "    if idx == 2:\n",
    "        break\n",
    "\n",
    "text_chunks = [d.page_content for d in text_chunks]\n",
    "print(\"Length of splits (text-heay case): \" + str(len(text_chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Image-heavy\n",
    "Image-heavy PDF can be converted the entire page to images and let a multimodal LLM like GPT-4o summarize each page.\n",
    "\n",
    "### Preprocess Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder './pdf_image_tmp' and its contents have been deleted.\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"./pdf_image_tmp\"\n",
    "delete_folder_and_make_folder(image_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from glob import glob\n",
    "\n",
    "pdf_image_path = f\"{splitted_raw_data_dir}/Image.pdf\"\n",
    "doc = fitz.open(pdf_image_path)\n",
    "#clip_x, clip_y = 10, 45\n",
    "clip_x, clip_y = 10, 10\n",
    "\n",
    "for i, page in enumerate(doc):\n",
    "    x, y, w, h = page.rect\n",
    "    clip = fitz.Rect(x+clip_x, y+clip_y, w-clip_x, h-clip_y)\n",
    "    page.set_cropbox(clip)\n",
    "    pix = page.get_pixmap()\n",
    "    pix.save(f\"{image_dir}/page_{i:03d}.jpg\")\n",
    "\n",
    "images = sorted(glob(os.path.join(image_dir, \"*.jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "max_tokens = 1024\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=max_tokens,\n",
    "    openai_api_version=\"2024-05-01-preview\",\n",
    "    azure_deployment=\"gpt-4o\"                       \n",
    ")\n",
    "\n",
    "system_prompt = \"You are an assistant tasked with describing table or image, specialized in Smartphone product.\"\n",
    "system_message_template = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "human_prompt = [\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": \"data:image/png;base64,\" + \"{image_base64}\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": '''Given image, give a concise summary in Korean. Don't insert any XML tag such as <text> and </text> when answering.'''\n",
    "    },\n",
    "]\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message_template,\n",
    "        human_message_template\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = prompt | llm | StrOutputParser()\n",
    "#summarize_chain = {\"image_base64\": lambda x:x} | prompt | llm_text | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 ms, sys: 4.7 ms, total: 15.6 ms\n",
      "Wall time: 4.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from util.preprocess import encode_image_base64\n",
    "#images = glob(os.path.join(image_path, \"*.jpg\"))\n",
    "base64_images = [encode_image_base64(img_path) for img_path in images]\n",
    "image_summaries = summarize_chain.batch(base64_images, {\"max_concurrency\": 8})\n",
    "image_summaries = remove_short_sentences(image_summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of image_summaries (image-heavy case): 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of image_summaries (image-heavy case): \" + str(len(image_summaries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct QnA Pairs\n",
    "----\n",
    "\n",
    "### Option 1. \n",
    "Leverage the azure-ai-generative package. The QADataGenerator class in this package makes it easy to generate QnA synthetic questions. However, using this class as is has the disadvantage of not being able to use custom prompts, so we inherited from it and created the CustomQADataGenerator class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.qa import CustomQADataGenerator\n",
    "model_config = {\n",
    "    \"deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"max_tokens\": 2000,\n",
    "}\n",
    "\n",
    "qa_generator = CustomQADataGenerator(model_config=model_config, templates_dir=\"./prompt_template/ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "import os\n",
    "from azure.ai.generative.synthetic.qa import QAType\n",
    "concurrency = 6  # number of concurrent calls\n",
    "sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "#qa_type = QAType.CONVERSATION\n",
    "qa_type = QAType.LONG_ANSWER\n",
    "\n",
    "async def generate_async(text: str) -> Dict:\n",
    "    async with sem:\n",
    "        return await qa_generator.generate_async(\n",
    "            text=text,\n",
    "            qa_type=qa_type,\n",
    "            num_questions=3,  # Number of questions to generate per text\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated QAs\n"
     ]
    }
   ],
   "source": [
    "input_batch = mixed_chunks + text_chunks + image_summaries\n",
    "results = await asyncio.gather(*[generate_async(text) for text in input_batch], return_exceptions=True)\n",
    "\n",
    "question_answer_list = []\n",
    "token_usage = Counter()\n",
    "for result in results:\n",
    "    if isinstance(result, Exception):\n",
    "        raise result  # exception raised inside generate_async()\n",
    "    question_answer_list.append(result[\"question_answers\"])\n",
    "    token_usage += result[\"token_usage\"]\n",
    "\n",
    "print(\"Successfully generated QAs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('이 이미지는 무엇을 보여주고 있습니까?', '이 이미지는 스마트폰의 후면을 보여주며, 각 구성 요소의 위치를 설명하고 있습니다.\\n'),\n",
       " ('이미지에서 표시된 구성 요소들은 무엇입니까?',\n",
       "  '이미지에서 표시된 구성 요소들은 후면 카메라, GPS 안테나, NFC 안테나, MST 안테나/무선 충전 코일입니다.\\n'),\n",
       " ('MST 안테나와 무선 충전 코일의 위치는 어떻게 표시되어 있습니까?',\n",
       "  'MST 안테나와 무선 충전 코일의 위치가 함께 표시되어 있습니다.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answer_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. \n",
    "You write the entire sequence of code to create a QnA dataset without using a separate toolkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from util.qa_pair import get_qna_prompt_template, QAPair\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=1024,\n",
    "    openai_api_version=aoai_api_version,\n",
    "    azure_deployment=aoai_deployment_name                    \n",
    ")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=QAPair)\n",
    "prompt = get_qna_prompt_template()\n",
    "#prompt = get_qna_repair_cost_prompt_template()\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = []\n",
    "\n",
    "for doc in mixed_chunks:\n",
    "    dic = {\"context\": doc, \"domain\": \"Samsung Galaxy S series Smartphone, especially S23 and S24 series\", \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)\n",
    "\n",
    "for doc in text_chunks:\n",
    "    dic = {\"context\": doc, \"domain\": \"Samsung Galaxy S series Smartphone, especially S23 and S24 series\", \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)\n",
    "\n",
    "for doc in image_summaries:\n",
    "    dic = {\"context\": doc, \"domain\": \"Samsung Galaxy S series Smartphone, especially S23 and S24 series\", \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 191 ms, sys: 30.8 ms, total: 222 ms\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#input_query = {\"context\": md_content, \"domain\": \"Samsung Galaxy S series Smartphone\", \"num_questions\": \"3\"}\n",
    "qa_pair = chain.batch(input_batch, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save to jsonl for fine-tuning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from util.common_utils import convert_to_oai_format, save_jsonl\n",
    "\n",
    "output_dir = './dataset'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "system_prompt_msg = \"\"\"You are an AI assistant that provides guidance to help users self-service resolve abnormalities in their Galaxy mobile phone.\\n\n",
    "Please answer the questions accurately. If the question is in Korean, write your answer in Korean. If the question is in English, write your answer in English.\"\"\"\n",
    "\n",
    "save_filename = \"cs-self-solve\"\n",
    "oai_qa_pair = convert_to_oai_format(question_answer_list)\n",
    "\n",
    "#save_jsonl(qa_pair, f\"{output_dir}/{save_filename}.jsonl\")\n",
    "save_jsonl(oai_qa_pair, f\"{output_dir}/{save_filename}-oai.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {splitted_raw_data_dir} pdf_image_tmp pdf_mixed_tmp outputs_tmp images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
