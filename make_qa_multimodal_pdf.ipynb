{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyMuPDF\n",
    "import fitz\n",
    "raw_data_dir = \"raw_data\"\n",
    "\n",
    "file_path = f\"{raw_data_dir}/prod-unst-pdf/SM-S92X_UG_UU_Kor_Rev.1.1_240129.pdf\"\n",
    "# Open the first PDF document\n",
    "doc1 = fitz.open(file_path)\n",
    "split_pages = [(10, 15)]\n",
    "\n",
    "for idx, s in enumerate(split_pages):\n",
    "    # Create a new empty PDF document\n",
    "    doc2 = fitz.open()\n",
    "\n",
    "    # Insert the first 2 pages of doc1 into doc2\n",
    "    doc2.insert_pdf(doc1, from_page=s[0], to_page=s[1])\n",
    "\n",
    "    # Save the modified document\n",
    "    doc2.save(f\"{raw_data_dir}/prod-unst-pdf/s24-user-manual-part{idx}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, shutil, random\n",
    "from unstructured.cleaners.core import clean_bullets, clean_extra_whitespace, remove_punctuation\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader, UnstructuredMarkdownLoader, UnstructuredAPIFileLoader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader, UnstructuredCSVLoader\n",
    "from util.preprocess import remove_short_sentences, remove_small_images\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "file_path = f\"{raw_data_dir}/prod-unst-pdf/s24-user-manual-part{idx}.pdf\"\n",
    "image_path = \"./image\"\n",
    "\n",
    "if os.path.isdir(image_path): shutil.rmtree(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Read & Preprocess PDF file\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 s, sys: 4.03 s, total: 40 s\n",
      "Wall time: 40.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chunk_size = 1500\n",
    "new_after_n_chars = 1200\n",
    "combine_text_under_n_chars = 1000\n",
    "chunk_overlap = 100\n",
    "max_tokens = 1024\n",
    "\n",
    "loader = UnstructuredFileLoader(\n",
    "    file_path=file_path,\n",
    "\n",
    "    chunking_strategy = \"by_title\",\n",
    "    mode=\"elements\",\n",
    "\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    hi_res_model_name=\"yolox_quantized\", #\"detectron2_onnx\", \"yolox\", \"yolox_quantized\"\n",
    "\n",
    "    extract_images_in_pdf=True,\n",
    "    #skip_infer_table_types='[]', # ['pdf', 'jpg', 'png', 'xls', 'xlsx', 'heic']\n",
    "    skip_infer_table_types=True, ## enable to get table as html using tabletrasformer\n",
    "\n",
    "    extract_image_block_output_dir=image_path,\n",
    "    extract_image_block_to_payload=False, ## False: to save image\n",
    "\n",
    "    max_characters=chunk_size,\n",
    "    new_after_n_chars=new_after_n_chars,\n",
    "    combine_text_under_n_chars=combine_text_under_n_chars, # 이 문자 수 이하의 텍스트는 결합\n",
    "\n",
    "    languages= [\"kor+eng\"],\n",
    "\n",
    "    post_processors=[clean_bullets, clean_extra_whitespace, remove_punctuation]\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "# from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "# from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "# from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "# # Initiate Azure AI Document Intelligence to load the document. You can either specify file_path or url_path to load the document.\n",
    "# loader = AzureAIDocumentIntelligenceLoader(\n",
    "#     file_path=file_path, \n",
    "#     api_key=os.getenv(\"AZURE_DOC_INTELLIGENCE_API_KEY\"), \n",
    "#     api_endpoint=os.getenv(\"AZURE_DOC_INTELLIGENCE_ENDPOINT\"), \n",
    "#     api_model=\"prebuilt-layout\"\n",
    "# )\n",
    "# docs = loader.load()\n",
    "\n",
    "# # Split the document into chunks base on markdown headers.\n",
    "# headers_to_split_on = [\n",
    "#     (\"#\", \"Header 1\"),\n",
    "#     (\"##\", \"Header 2\"),\n",
    "#     (\"###\", \"Header 3\"),\n",
    "#     (\"####\", \"Header 4\"),\n",
    "# ]\n",
    "# text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# docs_string = docs[0].page_content\n",
    "# splits = text_splitter.split_text(docs_string)\n",
    "\n",
    "# print(\"Length of splits: \" + str(len(splits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # texts: 5 \n",
      " # tables: 0 \n",
      " # images: 6\n"
     ]
    }
   ],
   "source": [
    "images = remove_small_images(image_path, image_dim_thres=16)\n",
    "tables, texts = [], []\n",
    "\n",
    "for doc in docs:\n",
    "    category = doc.metadata[\"category\"]\n",
    "    if category == \"Table\": tables.append(doc)\n",
    "    else: texts.append(doc)\n",
    "\n",
    "print (f' # texts: {len(texts)} \\n # tables: {len(tables)} \\n # images: {len(images)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=max_tokens,\n",
    "    openai_api_version=\"2024-05-01-preview\",\n",
    "    azure_deployment=\"gpt-4o\"                       \n",
    ")\n",
    "\n",
    "system_prompt = \"You are an assistant tasked with describing table or image, specialized in Smartphone product.\"\n",
    "system_message_template = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "human_prompt = [\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": \"data:image/png;base64,\" + \"{image_base64}\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": '''Given image, give a concise summary in Korean. Don't insert any XML tag such as <text> and </text> when answering.'''\n",
    "    },\n",
    "]\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message_template,\n",
    "        human_message_template\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = prompt | llm | StrOutputParser()\n",
    "#summarize_chain = {\"image_base64\": lambda x:x} | prompt | llm_text | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 67.8 ms, sys: 12.2 ms, total: 80 ms\n",
      "Wall time: 5.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from util.preprocess import encode_image_base64\n",
    "#images = glob(os.path.join(image_path, \"*.jpg\"))\n",
    "base64_images = [encode_image_base64(img_path) for img_path in images]\n",
    "image_summaries = summarize_chain.batch(base64_images, {\"max_concurrency\": 3})\n",
    "image_summaries = remove_short_sentences(image_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이 이미지는 스마트폰의 SIM 카드 트레이에 SIM 카드를 삽입하고 트레이를 스마트폰에 다시 넣는 방법을 보여줍니다. 첫 번째 그림에서는 SIM 카드를 트레이에 넣는 방법을, 두 번째 그림에서는 트레이를 스마트폰에 삽입하는 방법을 설명하고 있습니다.',\n",
       " '이 이미지는 스마트폰의 SIM 카드 트레이를 꺼내는 방법을 보여줍니다. 첫 번째 단계에서는 핀 도구를 사용하여 트레이 옆의 작은 구멍에 삽입합니다. 두 번째 단계에서는 트레이가 튀어나오면 손으로 잡아 빼냅니다.',\n",
       " '이미지는 스마트폰의 SIM 카드 슬롯을 여는 방법을 보여줍니다. 핀 도구를 사용하여 슬롯 옆의 작은 구멍에 넣고 눌러서 슬롯을 엽니다.',\n",
       " '이 이미지는 스마트폰 또는 태블릿의 충전 포트에 충전 케이블을 연결하는 방법을 보여줍니다.',\n",
       " \"이 이미지는 스마트폰의 하단 버튼을 설명하고 있습니다. 왼쪽부터 '최근 앱 버튼', '홈 버튼', '뒤로가기 버튼'이 있습니다.\"]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.preprocess import split_text_using_tiktoken\n",
    "\n",
    "texts_tiktoken = split_text_using_tiktoken(texts, chunk_size, chunk_overlap)\n",
    "image_summaries_tiktoken = split_text_using_tiktoken(image_summaries, chunk_size, chunk_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct QnA Pairs\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from util.qa_pair import get_qna_prompt_template, QAPair\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=1024,\n",
    "    openai_api_version=\"2024-05-01-preview\",\n",
    "    azure_deployment=\"gpt-4o\"                       \n",
    ")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=QAPair)\n",
    "prompt = get_qna_prompt_template()\n",
    "#prompt = get_qna_repair_cost_prompt_template()\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = []\n",
    "\n",
    "for doc in texts_tiktoken:\n",
    "    dic = {\"context\": doc, \"domain\": \"Samsung Galaxy S series Smartphone\", \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)\n",
    "\n",
    "#for doc in image_summaries_tiktoken:\n",
    "for doc in image_summaries:\n",
    "    dic = {\"context\": doc, \"domain\": \"Samsung Galaxy S series Smartphone\", \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 91.1 ms, sys: 9.84 ms, total: 101 ms\n",
      "Wall time: 9.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qa_pair = chain.batch(input_batch, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save to jsonl for fine-tuning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from util.common_utils import convert_to_oai_format, save_jsonl\n",
    "\n",
    "output_dir = './dataset'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "system_prompt_msg = \"\"\"You are an AI assistant that provides guidance to help users self-service resolve abnormalities in their Galaxy mobile phone.\\n\n",
    "Please answer the questions accurately. If the question is in Korean, write your answer in Korean. If the question is in English, write your answer in English.\"\"\"\n",
    "\n",
    "save_filename = \"cs-self-solve\"\n",
    "oai_qa_pair = convert_to_oai_format(qa_pair)\n",
    "\n",
    "s#ave_jsonl(qa_pair, f\"{output_dir}/{save_filename}.jsonl\")\n",
    "save_jsonl(oai_qa_pair, f\"{output_dir}/{save_filename}-oai.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
