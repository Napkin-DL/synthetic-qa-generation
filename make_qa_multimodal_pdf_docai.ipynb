{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate QnA synthetic dataset from a Complex PDF using Azure AI Document Intelligence\n",
    "\n",
    "### Overview\n",
    "We process the PDF by dividing it into three parts.\n",
    "\n",
    "- **Text-heavy** - Text-heavy PDF can be processed with open source without the need to use toolkits like Azure AI Document Intelligence or Unstructured.\n",
    "- **Image-heavy** - Image-heavy PDF can be converted the entire page to images and let a multimodal LLM like GPT-4o summarize each page.\n",
    "- **Mixed** - After reading the document with Azure AI Document Intelligence, we replace the image descriptions inside the figure tags with text summarized by a multimodal LLM. (Often the image descriptions are blank or have only a short caption.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read & Preprocess PDF file\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the PDFs into individual pages\n",
    "Only use a poration of the PDF documents for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, shutil, random\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "raw_data_dir = \"raw_data\"\n",
    "splitted_raw_data_dir = f\"splitted_{raw_data_dir}\"\n",
    "\n",
    "#file_path = f\"{raw_data_dir}/prod-unst-pdf/[Sales Talk] 3. QnA3_Handling Objection_(S24)_240227.pdf\"\n",
    "file_path = f\"{raw_data_dir}/prod-unst-pdf/SM-S92X_UG_UU_Kor_Rev.1.1_240129.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# Open the first PDF document\n",
    "doc1 = fitz.open(file_path)\n",
    "split_pages = [(1, 15)]\n",
    "\n",
    "for idx, s in enumerate(split_pages):\n",
    "    # Create a new empty PDF document\n",
    "    doc2 = fitz.open()\n",
    "\n",
    "    # Insert the first 2 pages of doc1 into doc2\n",
    "    doc2.insert_pdf(doc1, from_page=s[0], to_page=s[1])\n",
    "\n",
    "    # Save the modified document\n",
    "    doc2.save(f\"{raw_data_dir}/prod-unst-pdf/s24-user-manual-part{idx}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PDF Content Analysis Result:\n",
      "Text pages: [0, 1, 8, 10, 12, 13]\n",
      "Mixed pages: [2, 3, 4, 5, 6, 7, 11, 14]\n",
      "Image pages: [9]\n"
     ]
    }
   ],
   "source": [
    "from util.common_utils import delete_folder_and_make_folder\n",
    "from util.preprocess import remove_short_sentences, remove_small_images, analyze_pdf_page_content, split_pdf\n",
    "\n",
    "file_path = f\"{raw_data_dir}/prod-unst-pdf/s24-user-manual-part0.pdf\"\n",
    "result = analyze_pdf_page_content(file_path)\n",
    "delete_folder_and_make_folder(splitted_raw_data_dir)    \n",
    "\n",
    "print(\"### PDF Content Analysis Result:\")\n",
    "for content_type, pages in result.items():\n",
    "    print(f\"{content_type} pages: {pages}\")\n",
    "    split_pdf(file_path, f\"{splitted_raw_data_dir}/{content_type}.pdf\", pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import ContentFormat\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "doc_intelligence_endpoint = os.getenv(\"AZURE_DOC_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOC_INTELLIGENCE_KEY\")\n",
    "\n",
    "document_intelligence_client = DocumentIntelligenceClient(\n",
    "    endpoint=doc_intelligence_endpoint, \n",
    "    credential=AzureKeyCredential(doc_intelligence_key),\n",
    "    headers={\"x-ms-useragent\":\"sample-code-figure-understanding/1.0.0\"},\n",
    ")\n",
    "\n",
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=aoai_api_key,  \n",
    "    api_version=aoai_api_version,\n",
    "    base_url=f\"{aoai_api_endpoint}/openai/deployments/{aoai_deployment_name}\",\n",
    "    max_retries=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Mixed page (Images and text mixed appropriately)\n",
    "After reading the document with Azure AI Document Intelligence, we replace the image descriptions inside the figure tags with text summarized by a multimodal LLM. (Often the image descriptions are blank or have only a short caption.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_mixed_path = f\"{splitted_raw_data_dir}/Mixed.pdf\"\n",
    "\n",
    "with open(pdf_mixed_path, \"rb\") as f:\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\", analyze_request=f, content_type=\"application/octet-stream\", \n",
    "        output_content_format=ContentFormat.MARKDOWN \n",
    "    )\n",
    "\n",
    "result = poller.result()\n",
    "md_content = result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updates the content of the figure description (empty content or caption) with the image summary text generated by gpt-4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figures:\n",
      "4.7014 5.3766\n",
      "\tDescription of figure 0: 이미지에는 여러 기능과 애플리케이션 목록이 표시되어 있습니다. 항목에는 Samsung Notes, Samsung Members, Samsung Kids, Samsung Global Goals, Samsung Find, 삼성닷컴, Galaxy Wearable, PENUP, 캘린더, 리마인더, 라디오, 음성 녹음, 내 파일, 시계, 계산기, Gaming Hub, 게임 부스터, SmartThings, 콘텐츠 공유하기, Music Share, Smart View, Wi-Fi 다이렉트 등이 있습니다.\n",
      "\n",
      "이중 몇몇 항목들은 다음과 같이 한국어와 영어로 병기되어 있습니다:\n",
      "- Samsung Notes(삼성 노트)\n",
      "- 삼성메디아(Samsung Members)\n",
      "- Samsung Kids(삼성 키즈)\n",
      "- 삼성 글로벌 목표들(Samsung Global Goals)\n",
      "- Samsung Find(삼성 파인드)\n",
      "\n",
      "마지막으로, 이미지 하단에는 \"보안 접근성\"이라는 제목과 함께 \"보안 접근성\" 기능을 위한 정보가 나열되어 있습니다.\n",
      "5.693300000000001 1.6506000000000007\n",
      "\tDescription of figure 1: 이 이미지에는 텍스트가 나열되어 있습니다. 텍스트는 다음과 같습니다:\n",
      "\n",
      "1. Google(구글) 앱\n",
      "2. 이동통신 사업자 앱\n",
      "3. 인터넷\n",
      "4. Samsung Pay(삼성 페이)\n",
      "\n",
      "텍스트는 주로 영어와 한국어로 혼합되어 있습니다.\n",
      "4.5553 5.4625\n",
      "\tDescription of figure 2: 이 이미지에는 한국어로 작성된 목록이 보입니다. 일부 항목은 페이지 번호와 함께 나와 있으며, 이는 아마도 책이나 메뉴얼의 목차로 보입니다. \n",
      "\n",
      "목차의 일부 내용은 다음과 같습니다:\n",
      "\n",
      "- 절약 모드\n",
      "- 네트워크 설정\n",
      "- 듀얼 메신저\n",
      "- 디지털 웰빙 및 자녀 보호 기능\n",
      "- 디바이스 케어\n",
      "- 애플리케이션\n",
      "- 접근성\n",
      "- 소프트웨어 업데이트\n",
      "- 휴대전화 정보\n",
      "- 이동통신 사업자 설정\n",
      "\n",
      "또한, \"제품 사용 시 알아두기\"라는 제목 아래에 몇 가지 항목이 나열되어 있습니다:\n",
      "\n",
      "- 제품 사용 시 주의 사항\n",
      "- 구성품 및 별매품 안내 사항\n",
      "- 제품 방수 및 방진 기능 주의사항\n",
      "- 제품에서 열이 발생하는 경우\n",
      "- 개인정보 및 제품 분실 시 대비\n",
      "\n",
      "등의 항목이 포함되어 있습니다.\n",
      "5.158 7.138400000000001\n",
      "\tDescription of figure 3: 이 이미지는 스마트폰의 각 기능과 위치를 설명하는 다이어그램입니다. 다이어그램에는 다음과 같은 부품들이 표시되어 있습니다:\n",
      "\n",
      "- 수화부/스피커: 스마트폰 상단에 위치\n",
      "- 전면 카메라: 스마트폰 상단 중앙\n",
      "- 근접/조도 센서: 전면 카메라 근처\n",
      "- 화면: 스마트폰의 중앙 넓은 부분\n",
      "- 마이크: 스마트폰 상단\n",
      "- 에어 벤트 홈: 스마트폰 상단\n",
      "- 음량 버튼: 스마트폰의 오른쪽 중간\n",
      "- 측면 버튼: 음량 버튼 아래\n",
      "- 지문 인식 센서: 화면 하단의 중앙 근처\n",
      "\n",
      "이 설명은 스마트폰 부품들의 위치와 역할을 시각적으로 안내합니다.\n",
      "4.532 4.9466\n",
      "\tDescription of figure 4: 이 이미지는 스마트폰 뒷면의 구성 요소를 설명하고 있습니다. 각 구성 요소는 다음과 같습니다:\n",
      "\n",
      "- Laser AF 센서\n",
      "- 마이크\n",
      "- 후면 카메라\n",
      "- 플래시\n",
      "- GPS 안테나\n",
      "- NFC 안테나\n",
      "- MST 안테나/무선 충전 코일\n",
      "\n",
      "각 구성 요소는 스마트폰의 기능을 향상시키기 위해 설계되었습니다. 각 라벨과 연결선은 해당 구성 요소의 위치를 나타내고 있습니다.\n",
      "5.1497 7.0931999999999995\n",
      "\tDescription of figure 5: 이 이미지는 스마트폰의 주요 구성 요소를 나타내고 있습니다. 각 구성 요소는 다음과 같습니다:\n",
      "\n",
      "1. 수화부/스피커\n",
      "2. 전면 카메라\n",
      "3. 근접/조도 센서\n",
      "4. 화면\n",
      "5. 지문 인식 센서\n",
      "6. 마이크\n",
      "7. 에어 벤트 홈\n",
      "8. 음량 버튼\n",
      "9. 측면 버튼\n",
      "10. 트레이 홈\n",
      "11. SIM 카드 트레이\n",
      "12. 마이크\n",
      "13. 스피커\n",
      "14. 이어폰/외부 커넥터 연결잭(USB Type-C)\n",
      "\n",
      "각 구성 요소는 이미지에서 해당 위치를 나타내는 선과 함께 레이블이 붙어 있습니다.\n",
      "4.5294 4.938600000000001\n",
      "\tDescription of figure 6: 이 이미지는 스마트폰의 후면 디자인과 구성요소를 설명하고 있습니다. 주요 컴포넌트는 다음과 같습니다:\n",
      "\n",
      "- 마이크: 기기의 왼쪽 상단에 위치.\n",
      "- 플래시: 스마트폰 후면 상단 중앙에 위치.\n",
      "- 후면 카메라: 세 개의 렌즈가 기기의 왼쪽 상단에 세로로 배열되어 있습니다.\n",
      "- GPS 안테나: 기기의 오른쪽 상단에 위치.\n",
      "- NFC 안테나: GPS 안테나 바로 아래에 위치.\n",
      "- MST 안테나/무선 충전 코일: 기기 뒷면 중앙에 위치.\n",
      "- 메인 안테나: 기기의 하단 오른쪽에 위치.\n",
      "\n",
      "이 이미지에서는 각 컴포넌트의 위치가 파란 선으로 표시되어 있습니다.\n",
      "6.6477 1.9201999999999995\n",
      "simple image at idx 7\n",
      "6.5632 2.8611\n",
      "\tDescription of figure 8: 이 이미지는 스마트폰의 뒷면을 보여주는 도면입니다. 도면에는 다음과 같은 부위들이 표시되어 있습니다:\n",
      "\n",
      "- 마이크\n",
      "- 플래시\n",
      "- 후면 카메라\n",
      "- GPS 안테나\n",
      "- NFC 안테나\n",
      "\n",
      "각 부위는 파란색 선으로 연결되어 명칭이 붙어 있습니다. 후면에는 여러 개의 카메라 렌즈가 배열되어 있습니다.\n",
      "6.6915 2.8623000000000003\n",
      "\tDescription of figure 9: 이 이미지는 스마트폰의 특정 부위들을 나타내는 도해입니다. 이미지의 왼쪽 상단에는 MST 안테나/무선 충전 코일이 표시되어 있고, 이미지의 오른쪽 하단에는 메인 안테나가 표시되어 있습니다. 각 부위는 점선으로 표시되어 있으며, 해당 부위는 스마트폰의 특정 위치를 강조하고 있습니다.\n",
      "CPU times: user 1.25 s, sys: 160 ms, total: 1.41 s\n",
      "Wall time: 40.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from util.preprocess import (\n",
    "    image_complexity, is_bounding_box_larger_than, crop_image_from_file, \n",
    "    understand_image_with_gpt, update_figure_description\n",
    ")\n",
    "output_folder = \"pdf_mixed_tmp\"\n",
    "delete_folder_and_make_folder(output_folder)\n",
    "language = \"Korean\"\n",
    "max_tokens = 1024\n",
    "input_file_path = file_path\n",
    "\n",
    "if result.figures:\n",
    "    print(\"Figures:\")\n",
    "    for idx, figure in enumerate(result.figures):\n",
    "        figure_content = \"\"\n",
    "        img_description = \"\"\n",
    "        #print(f\"Figure #{idx} has the following spans: {figure.spans}\")\n",
    "        \n",
    "        for i, span in enumerate(figure.spans):\n",
    "            #print(f\"Span #{i}: {span}\")\n",
    "            figure_content += md_content[span.offset:span.offset + span.length]\n",
    "        #print(f\"Original figure content in markdown: {figure_content}\")\n",
    "\n",
    "        # Note: figure bounding regions currently contain both the bounding region of figure caption and figure body\n",
    "        if figure.caption:\n",
    "            caption_region = figure.caption.bounding_regions\n",
    "            #print(f\"\\tCaption: {figure.caption.content}\")\n",
    "            #print(f\"\\tCaption bounding region: {caption_region}\")\n",
    "            for region in figure.bounding_regions:\n",
    "                if region not in caption_region:\n",
    "                    #print(f\"\\tFigure body bounding regions: {region}\")\n",
    "                    # To learn more about bounding regions, see https://aka.ms/bounding-region\n",
    "                    boundingbox = (\n",
    "                            region.polygon[0],  # x0 (left)\n",
    "                            region.polygon[1],  # y0 (top)\n",
    "                            region.polygon[4],  # x1 (right)\n",
    "                            region.polygon[5]   # y1 (bottom)\n",
    "                        )\n",
    "\n",
    "                    if is_bounding_box_larger_than(boundingbox):\n",
    "                        #print(f\"\\tFigure body bounding box in (x0, y0, x1, y1): {boundingbox}\")\n",
    "                        cropped_image = crop_image_from_file(input_file_path, region.page_number - 1, boundingbox) # page_number is 1-indexed\n",
    "\n",
    "                        if image_complexity(cropped_image)[0] == \"Complex\":\n",
    "                            # Get the base name of the file\n",
    "                            base_name = os.path.basename(input_file_path)\n",
    "                            # Remove the file extension\n",
    "                            file_name_without_extension = os.path.splitext(base_name)[0]\n",
    "\n",
    "                            output_file = f\"{file_name_without_extension}_cropped_image_{idx}.png\"\n",
    "                            cropped_image_filename = os.path.join(output_folder, output_file)\n",
    "\n",
    "                            cropped_image.save(cropped_image_filename)\n",
    "                            print(f\"\\tFigure {idx} cropped and saved as {cropped_image_filename}\")\n",
    "\n",
    "                            try: \n",
    "                                image_summarization = understand_image_with_gpt(client, aoai_deployment_name, cropped_image_filename, \"\", max_tokens=max_tokens, language=language)\n",
    "                            except openai.BadRequestError as e:\n",
    "                                print(f\"BadRequestError: {e}\")\n",
    "                                image_summarization = \"\"\n",
    "                            img_description += image_summarization\n",
    "\n",
    "                            print(f\"\\tDescription of figure {idx}: {img_description}\")\n",
    "                        else:\n",
    "                            print(f'simple image at idx {idx}')\n",
    "\n",
    "        else:\n",
    "            #print(\"\\tNo caption found for this figure.\")\n",
    "            for region in figure.bounding_regions:\n",
    "                #print(f\"\\tFigure body bounding regions: {region}\")\n",
    "                # To learn more about bounding regions, see https://aka.ms/bounding-region\n",
    "                boundingbox = (\n",
    "                        region.polygon[0],  # x0 (left)\n",
    "                        region.polygon[1],  # y0 (top\n",
    "                        region.polygon[4],  # x1 (right)\n",
    "                        region.polygon[5]   # y1 (bottom)\n",
    "                    )\n",
    "\n",
    "                if is_bounding_box_larger_than(boundingbox):                    \n",
    "                    #print(f\"\\tFigure body bounding box in (x0, y0, x1, y1): {boundingbox}\")\n",
    "\n",
    "                    cropped_image = crop_image_from_file(input_file_path, region.page_number - 1, boundingbox) # page_number is 1-indexed\n",
    "\n",
    "                    if image_complexity(cropped_image)[0] == \"Complex\":\n",
    "                        # Get the base name of the file\n",
    "                        base_name = os.path.basename(input_file_path)\n",
    "                        # Remove the file extension\n",
    "                        file_name_without_extension = os.path.splitext(base_name)[0]\n",
    "\n",
    "                        output_file = f\"{file_name_without_extension}_cropped_image_{idx}.png\"\n",
    "                        cropped_image_filename = os.path.join(output_folder, output_file)\n",
    "                        # cropped_image_filename = f\"data/cropped/image_{idx}.png\"\n",
    "                        cropped_image.save(cropped_image_filename)\n",
    "                        #print(f\"\\tFigure {idx} cropped and saved as {cropped_image_filename}\")\n",
    "\n",
    "                        try:\n",
    "                            image_summarization = understand_image_with_gpt(client, aoai_deployment_name, cropped_image_filename, \"\", max_tokens=max_tokens, language=language)\n",
    "                        except openai.BadRequestError as e:\n",
    "                            print(f\"BadRequestError: {e}\")\n",
    "                            image_summarization = \"\"\n",
    "                        img_description += image_summarization\n",
    "                        print(f\"\\tDescription of figure {idx}: {img_description}\")\n",
    "                    else:\n",
    "                        print(f'simple image at idx {idx}')\n",
    "\n",
    "        \n",
    "        md_content = update_figure_description(md_content, img_description, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, Markdown, Latex\n",
    "# display(Markdown(md_content[:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate chunks for mixed pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of splits (mixed case): 3\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        r'<!-- PageNumber=\"\\d+\" -->',\n",
    "        r\"\\n\\n\",\n",
    "        r\"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "        \"\",\n",
    "    ],   \n",
    "    is_separator_regex = True,    \n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "mixed_chunks = text_splitter.split_text(md_content)\n",
    "print(\"Length of splits (mixed case): \" + str(len(mixed_chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Text-heavy\n",
    "Text-heavy PDFs can be processed with open source without the need to use toolkits like Azure AI Document Intelligence or Unstructured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n",
      "page_content='2\\n시작하기\\n4\\t\\n각 부분의 이름과 역할\\n11\\t\\n배터리 충전하기\\n15\\t\\nNano-SIM 카드 및 eSIM\\n19\\t\\n전원 켜기/끄기\\n20\\t\\n제품 초기 설정\\n20\\t\\n삼성 계정\\n20\\t\\n이전 기기의 데이터 가져오기(Smart Switch)\\n22\\t\\n화면 알아두기\\n30\\t\\n알림창\\n31\\t\\n화면 캡처 및 화면 녹화\\n33\\t\\n문자 입력\\n35\\t\\n텍스트 추출\\n앱과 기능 알아보기\\n36\\t\\n앱 설치 및 관리\\n37\\t\\nS펜(Galaxy S24 Ultra)\\n50\\t\\n전화\\n54\\t\\n연락처\\n56\\t\\n메시지\\n58\\t\\n카메라\\n74\\t\\n갤러리\\n79\\t\\nAR 존\\n84\\t\\n빅스비\\n85\\t\\n빅스비 비전\\n86\\t\\n멀티윈도우(여러 앱 동시에 사용하기)\\n89\\t\\n삼성 인터넷\\n90\\t\\nSamsung Pay(삼성 페이)\\n차례\\n93\\t\\n삼성 헬스\\n94\\t\\nSamsung Notes(삼성 노트)\\n98\\t\\nSamsung Members(삼성 멤버스)\\n99\\t\\nSamsung Kids(삼성 키즈)\\n99\\t\\nSamsung Global Goals(삼성 글로벌 골)\\n99\\t\\nSamsung Find(삼성 파인드)\\n100\\t 삼성닷컴\\n100\\t Galaxy Wearable(갤럭시 웨어러블)\\n100\\t PENUP(펜업)(Galaxy S24 Ultra)\\n100\\t 캘린더\\n101\\t\\n리마인더(할 일 알림 받기)\\n102\\t 라디오\\n103\\t 음성 녹음\\n104\\t 내 파일(파일 확인 및 관리하기)\\n104\\t 시계\\n105\\t 계산기\\n105\\t Gaming Hub(게이밍 허브)\\n106\\t 게임 부스터(게임 환경 설정하기)\\n107\\t\\nSmartThings(스마트싱스)\\n108\\t 콘텐츠(파일) 공유하기\\n109\\t Music Share(뮤직 쉐어)\\n110\\t\\nSmart View(스마트 뷰)(TV 화면으로 보기)\\n111\\t\\nWindows와 연결(컴퓨터와 연결해 사용하기)\\n112\\t\\nSamsung DeX(삼성 덱스)\\n116\\t\\nGoogle(구글) 앱\\n117\\t\\n이동통신 사업자 앱' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 0, 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Chunk 1\n",
      "page_content='차례\\n3\\n설정 알아보기\\n121\\t\\n설정 소개\\n121\\t\\n삼성 계정\\n121\\t\\n연결\\n122\\t\\nWi-Fi\\n124\\t\\n블루투스\\n125\\t\\nNFC 및 비접촉 결제\\n126\\t\\n네트워크 상태 표시\\n126\\t\\n데이터 절약 모드\\n126\\t\\n앱별 네트워크 설정\\n127\\t\\n모바일 핫스팟\\n127\\t\\n기타 연결 설정\\n129\\t\\n기기 간 연결\\n130\\t 모드 및 루틴\\n130\\t 모드 사용하기\\n130\\t 루틴 사용하기\\n131\\t\\n소리 및 진동\\n132\\t\\n음질 및 음향 효과\\n132\\t\\n앱 소리 분리 재생\\n132\\t\\n알림\\n133\\t\\n디스플레이\\n134\\t\\n부드러운 모션 및 화면 전환\\n134\\t\\n화면 모드 변경 및 색상 조절\\n135\\t\\n배터리\\n136\\t\\n배경화면 및 스타일\\n136\\t\\n테마\\n136\\t\\n홈 화면\\n136\\t\\n잠금화면 및 AOD\\n137\\t\\n잠금 해제 유지\\n137\\t\\nAlways On Display(올웨이즈 온 \\n디스플레이)(꺼진 화면에 정보 표시하기)\\n138\\t 보안 및 개인정보 보호\\n139\\t\\n얼굴 인식\\n140\\t 지문 인식\\n142\\t\\n보안 폴더\\n144\\t 보안 Wi-Fi\\n145\\t\\nSamsung Pass(삼성 패스)\\n148\\t 위치\\n148\\t 안전 및 긴급\\n149\\t\\n계정 및 백업\\n149\\t\\n삼성 클라우드\\n150\\t Google(구글)\\n150\\t 유용한 기능\\n151\\t\\n향상된 인텔리전스\\n152\\t\\n모션 및 제스처\\n152\\t\\n영상통화 효과\\n153\\t\\n듀얼 메신저\\n153\\t\\n디지털 웰빙 및 자녀 보호 기능\\n154\\t\\n디바이스 케어\\n154\\t\\n애플리케이션\\n155\\t\\n일반\\n156\\t\\n접근성\\n156\\t\\n소프트웨어 업데이트\\n156\\t\\n휴대전화 정보\\n157\\t\\n이동통신 사업자 설정\\n제품 사용 시 알아두기\\n160\\t 제품 사용 시 주의 사항\\n162\\t\\n구성품 및 별매품 안내 사항\\n163\\t\\n제품 방수 및 방진 기능 주의 사항\\n164\\t\\n제품에서 열이 발생하는 경우 및 조치 방법\\n167\\t\\n개인정보 및 제품 분실 시 피해 방지 설정\\n부록\\n169\\t\\n접근성\\n181\\t\\n서비스를 요청하기 전에 확인할 사항\\n191\\t\\n규격 및 특성' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 1, 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Chunk 2\n",
      "page_content='시작하기\\n10\\n\\t\\n•\\n미디어 파일 재생, 스피커폰 통화 등 스피커를 사용하는 경우에는 제품을 귀에 가까이 대지 마세요.\\n\\t\\n•\\n직사광선 등 강한 광원에 카메라 렌즈가 노출되지 않도록 주의하세요. 직사광선과 같은 강한 광원에 \\n카메라 렌즈가 노출되면 카메라 이미지 센서가 손상될 수 있습니다. 손상된 이미지 센서는 회복되지 \\n않으며 그로 인해 촬영 시 점이나 얼룩이 생길 수 있습니다.\\n\\t\\n•\\n제품의 유리 또는 아크릴이 깨지거나 파손된 상태로 사용할 경우 다칠 위험이 있으니 반드시 삼성전자 \\n서비스 센터에서 수리 후 사용하세요.\\n\\t\\n•\\n마이크, 스피커 또는 수화부에 먼지나 이물질이 들어가거나 해당 부분이 가려지면 제품의 소리가 \\n작아지거나 특정 기능이 정상적으로 동작하지 않을 수 있습니다. 이때, 뾰족한 물체 등으로 먼지나 \\n이물질을 무리해서 제거하는 경우 제품이 손상될 수 있으니 주의하세요.\\n\\t\\n•\\n화면에는 지문 방지 코팅이 적용되어 있습니다. 이 코팅은 일반적으로 시간이 경과함에 따라 약화되며, \\n지속적인 압력이나 마찰이 가해질 경우 코팅 효과가 감소되거나 코팅 벗겨짐 등 손상이 발생될 수 \\n있습니다.\\n\\t\\n•\\n다음과 같은 경우에는 통화 품질 및 수신율이 떨어지거나, 배터리가 많이 소모될 수 있습니다.\\n\\t\\n－안테나 부분에 금속 재질의 스티커를 붙이는 경우\\n\\t\\n－금속 재질의 커버를 사용하는 경우\\n\\t\\n－통화나 모바일 데이터와 같은 제품의 기능 이용 중에 안테나 부분을 만질 경우\\n\\t\\n•\\n에어 벤트 홈이 보호 필름이나 스티커 등으로 가려져 있으면 통화 중이나 미디어 재생 시 잡음이 발생할 \\n수 있습니다.\\n\\t\\n•\\n근접/조도 센서 및 그 주변을 스티커나 커버 등의 액세서리로 가리지 마세요. 센서가 올바르게 \\n동작하지 않을 수 있습니다.\\n\\t\\n•\\n근접 센서의 동작으로 통화 시에는 제품 위쪽 부분에 불빛이 깜박일 수 있습니다.\\n하드웨어 버튼\\n버튼\\n기능\\n측면 버튼\\n\\t\\n•\\n제품이 꺼진 상태에서 길게 누르면 전원이 켜집니다.\\n\\t\\n•\\n짧게 누르면 화면이 켜지거나 잠깁니다.\\n\\t\\n•\\n길게 누르면 빅스비와 대화를 시작할 수 있습니다. 자세한 내용은 빅스비 \\n사용하기를 참고하세요.\\n\\t\\n•\\n두 번 누르거나 길게 누르면 설정해 놓은 앱 또는 기능을 실행할 수 \\n있습니다.\\n측면 버튼 + 음량(하) 버튼\\n\\t\\n•\\n동시에 짧게 누르면 화면을 캡처할 수 있습니다.\\n\\t\\n•\\n동시에 길게 누르면 전원을 끌 수 있습니다.' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 2, 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Length of splits (text-heay case): 7\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.pdf import PyMuPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_text_path = f\"{splitted_raw_data_dir}/Text.pdf\"\n",
    "loader = PyMuPDFLoader(pdf_text_path)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "for idx, chunk in enumerate(text_chunks):\n",
    "    print(f\"Chunk {idx}\\n{chunk}\")\n",
    "    print(\"=\"*80)\n",
    "    if idx == 2:\n",
    "        break\n",
    "\n",
    "text_chunks = [d.page_content for d in text_chunks]\n",
    "print(\"Length of splits (text-heay case): \" + str(len(text_chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Image-heavy\n",
    "Image-heavy PDF can be converted the entire page to images and let a multimodal LLM like GPT-4o summarize each page.\n",
    "\n",
    "### Preprocess Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"./pdf_image_tmp\"\n",
    "delete_folder_and_make_folder(image_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from glob import glob\n",
    "\n",
    "pdf_image_path = f\"{splitted_raw_data_dir}/Image.pdf\"\n",
    "doc = fitz.open(pdf_image_path)\n",
    "#clip_x, clip_y = 10, 45\n",
    "clip_x, clip_y = 10, 10\n",
    "\n",
    "for i, page in enumerate(doc):\n",
    "    x, y, w, h = page.rect\n",
    "    clip = fitz.Rect(x+clip_x, y+clip_y, w-clip_x, h-clip_y)\n",
    "    page.set_cropbox(clip)\n",
    "    pix = page.get_pixmap()\n",
    "    pix.save(f\"{image_dir}/page_{i:03d}.jpg\")\n",
    "\n",
    "images = sorted(glob(os.path.join(image_dir, \"*.jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "max_tokens = 1024\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=max_tokens,\n",
    "    openai_api_version=\"2024-05-01-preview\",\n",
    "    azure_deployment=\"gpt-4o\"                       \n",
    ")\n",
    "\n",
    "system_prompt = \"You are an assistant tasked with describing table or image, specialized in Smartphone product.\"\n",
    "system_message_template = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "human_prompt = [\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": \"data:image/png;base64,\" + \"{image_base64}\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": '''Given image, give a concise summary in Korean. Don't insert any XML tag such as <text> and </text> when answering.'''\n",
    "    },\n",
    "]\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message_template,\n",
    "        human_message_template\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = prompt | llm | StrOutputParser()\n",
    "#summarize_chain = {\"image_base64\": lambda x:x} | prompt | llm_text | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.1 ms, sys: 16.9 ms, total: 42 ms\n",
      "Wall time: 4.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from util.preprocess import encode_image_base64\n",
    "#images = glob(os.path.join(image_path, \"*.jpg\"))\n",
    "base64_images = [encode_image_base64(img_path) for img_path in images]\n",
    "image_summaries = summarize_chain.batch(base64_images, {\"max_concurrency\": 8})\n",
    "image_summaries = remove_short_sentences(image_summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of image_summaries (image-heavy case): 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of image_summaries (image-heavy case): \" + str(len(image_summaries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct QnA Pairs\n",
    "----\n",
    "\n",
    "### Option 1. \n",
    "Leverage the azure-ai-generative package. The QADataGenerator class in this package makes it easy to generate QnA synthetic questions. However, using this class as is has the disadvantage of not being able to use custom prompts, so we inherited from it and created the CustomQADataGenerator class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.qa import CustomQADataGenerator\n",
    "model_config = {\n",
    "    \"deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"max_tokens\": 2000,\n",
    "}\n",
    "\n",
    "qa_generator = CustomQADataGenerator(model_config=model_config, templates_dir=\"./prompt_template/ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "import os\n",
    "from azure.ai.generative.synthetic.qa import QAType\n",
    "concurrency = 6  # number of concurrent calls\n",
    "sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "#qa_type = QAType.CONVERSATION\n",
    "qa_type = QAType.LONG_ANSWER\n",
    "\n",
    "async def generate_async(text: str) -> Dict:\n",
    "    async with sem:\n",
    "        return await qa_generator.generate_async(\n",
    "            text=text,\n",
    "            qa_type=qa_type,\n",
    "            num_questions=3,  # Number of questions to generate per text\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated QAs\n"
     ]
    }
   ],
   "source": [
    "input_batch = mixed_chunks + text_chunks + image_summaries\n",
    "results = await asyncio.gather(*[generate_async(text) for text in input_batch], return_exceptions=True)\n",
    "\n",
    "question_answer_list = []\n",
    "token_usage = Counter()\n",
    "for result in results:\n",
    "    if isinstance(result, Exception):\n",
    "        raise result  # exception raised inside generate_async()\n",
    "    question_answer_list.append(result[\"question_answers\"])\n",
    "    token_usage += result[\"token_usage\"]\n",
    "\n",
    "print(\"Successfully generated QAs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Galaxy S24 Ultra의 이미지에는 어떤 기능과 애플리케이션 목록이 표시되어 있습니까?',\n",
       "  'Galaxy S24 Ultra의 이미지에는 Samsung Notes, Samsung Members, Samsung Kids, Samsung Global Goals, Samsung Find, 삼성닷컴, Galaxy Wearable, PENUP, 캘린더, 리마인더, 라디오, 음성 녹음, 내 파일, 시계, 계산기, Gaming Hub, 게임 부스터, SmartThings, 콘텐츠 공유하기, Music Share, Smart View, Wi-Fi 다이렉트 등의 기능과 애플리케이션 목록이 표시되어 있습니다.\\n'),\n",
       " ('Galaxy S24+의 다이어그램에는 어떤 부품들이 표시되어 있습니까?',\n",
       "  'Galaxy S24+의 다이어그램에는 수화부/스피커, 전면 카메라, 근접/조도 센서, 화면, 마이크, 에어 벤트 홈, 음량 버튼, 측면 버튼, 지문 인식 센서 등의 부품들이 표시되어 있습니다. 이 설명은 스마트폰 부품들의 위치와 역할을 시각적으로 안내합니다.\\n'),\n",
       " ('\"제품 사용 시 알아두기\" 항목에는 어떤 내용이 포함되어 있습니까?',\n",
       "  '\"제품 사용 시 알아두기\" 항목에는 제품 사용 시 주의 사항, 구성품 및 별매품 안내 사항, 제품 방수 및 방진 기능 주의사항, 제품에서 열이 발생하는 경우, 개인정보 및 제품 분실 시 대비 등의 내용이 포함되어 있습니다.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answer_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. \n",
    "You write the entire sequence of code to create a QnA dataset without using a separate toolkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from util.qa_pair import get_qna_prompt_template, QAPair\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=1024,\n",
    "    openai_api_version=aoai_api_version,\n",
    "    azure_deployment=aoai_deployment_name                    \n",
    ")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=QAPair)\n",
    "prompt = get_qna_prompt_template()\n",
    "#prompt = get_qna_repair_cost_prompt_template()\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = []\n",
    "\n",
    "for doc in mixed_chunks:\n",
    "    dic = {\"context\": doc, \"domain\": \"Samsung Galaxy S series Smartphone, especially S23 and S24 series\", \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)\n",
    "\n",
    "for doc in text_chunks:\n",
    "    dic = {\"context\": doc, \"domain\": \"Samsung Galaxy S series Smartphone, especially S23 and S24 series\", \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)\n",
    "\n",
    "for doc in image_summaries:\n",
    "    dic = {\"context\": doc, \"domain\": \"Samsung Galaxy S series Smartphone, especially S23 and S24 series\", \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 137 ms, sys: 17.3 ms, total: 154 ms\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#input_query = {\"context\": md_content, \"domain\": \"Samsung Galaxy S series Smartphone\", \"num_questions\": \"3\"}\n",
    "qa_pair = chain.batch(input_batch, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save to jsonl for fine-tuning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jsonl(qa_pair, f\"{output_dir}/{save_filename}.jsonl\")\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from util.common_utils import convert_to_oai_format, save_jsonl\n",
    "\n",
    "output_dir = './dataset'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "system_prompt_msg = \"\"\"You are an AI assistant that provides guidance to help users self-service resolve abnormalities in their Galaxy mobile phone.\\n\n",
    "Please answer the questions accurately. If the question is in Korean, write your answer in Korean. If the question is in English, write your answer in English.\"\"\"\n",
    "\n",
    "save_filename = \"cs-self-solve\"\n",
    "oai_qa_pair = convert_to_oai_format(question_answer_list)\n",
    "\n",
    "#save_jsonl(qa_pair, f\"{output_dir}/{save_filename}.jsonl\")\n",
    "save_jsonl(oai_qa_pair, f\"{output_dir}/{save_filename}-oai.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {splitted_raw_data_dir} pdf_image_tmp pdf_mixed_tmp outputs_tmp images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
