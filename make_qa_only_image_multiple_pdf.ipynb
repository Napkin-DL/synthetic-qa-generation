{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate QnA synthetic dataset from multiple PDFs - Image-heavy PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os, shutil, random\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from util.preprocess import convert_html_to_md, remove_short_sentences, remove_small_images\n",
    "import glob\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "\n",
    "raw_data_dir = \"raw_data\"\n",
    "csv_path = f\"{raw_data_dir}/prod-unst-pdf\"\n",
    "all_files = glob.glob(os.path.join(csv_path, \"[Sales*.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = all_files[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw_data/prod-unst-pdf/[Sales Guide] 1.Workshop Deck_(S23)_230201.pdf',\n",
       " 'raw_data/prod-unst-pdf/[Sales Talk] 4. Why Galaxy_(S24)_240116.pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import fitz\n",
    "from glob import glob\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "from util.preprocess import encode_image_base64\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from util.qa_pair import get_qna_prompt_template, QAPair\n",
    "from util.common_utils import convert_to_oai_format, save_jsonl\n",
    "\n",
    "max_tokens = 1024\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=max_tokens,\n",
    "    openai_api_version=\"2024-05-01-preview\",\n",
    "    azure_deployment=\"gpt-4o\"                       \n",
    ")\n",
    "\n",
    "system_prompt = \"You are an assistant tasked with describing table or image, specialized in Smartphone product.\"\n",
    "system_message_template = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "human_prompt = [\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": \"data:image/png;base64,\" + \"{image_base64}\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": '''Given image, give a concise summary in Korean. Don't insert any XML tag such as <text> and </text> when answering.'''\n",
    "    },\n",
    "]\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message_template,\n",
    "        human_message_template\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess each PDF file \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Idx 0 - Processing raw_data/prod-unst-pdf/[Sales Guide] 1.Workshop Deck_(S23)_230201.pdf...\n",
      "### Generating image summaries using LLM - path: raw_data/prod-unst-pdf/[Sales Guide] 1.Workshop Deck_(S23)_230201.pdf\n",
      "Elasped 75.40643 ses for generating image summaries using LLM\n",
      "### Generating QA pairs using LLM - path: raw_data/prod-unst-pdf/[Sales Guide] 1.Workshop Deck_(S23)_230201.pdf\n",
      "Elasped 54.81720 ses for generating image summaries using LLM\n",
      "### Saving QA pairs to jsonl\n",
      "\n",
      "##### Idx 1 - Processing raw_data/prod-unst-pdf/[Sales Talk] 4. Why Galaxy_(S24)_240116.pdf...\n",
      "### Generating image summaries using LLM - path: raw_data/prod-unst-pdf/[Sales Talk] 4. Why Galaxy_(S24)_240116.pdf\n",
      "Elasped 10.72489 ses for generating image summaries using LLM\n",
      "### Generating QA pairs using LLM - path: raw_data/prod-unst-pdf/[Sales Talk] 4. Why Galaxy_(S24)_240116.pdf\n",
      "Elasped 7.34492 ses for generating image summaries using LLM\n",
      "### Saving QA pairs to jsonl\n"
     ]
    }
   ],
   "source": [
    "for idx, file_path in enumerate(all_files):\n",
    "\n",
    "    print(f\"\\n##### Idx {idx} - Processing {file_path}...\")\n",
    "\n",
    "    image_path = \"./image\"\n",
    "    if os.path.isdir(image_path): shutil.rmtree(image_path)\n",
    "    os.makedirs(image_path, exist_ok=True)\n",
    "\n",
    "    doc = fitz.open(file_path)\n",
    "    doc.delete_page(0) # 1st page is the cover page, so we delete it.\n",
    "    #clip_x, clip_y = 10, 45\n",
    "    clip_x, clip_y = 10, 10\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        x, y, w, h = page.rect\n",
    "        clip = fitz.Rect(x+clip_x, y+clip_y, w-clip_x, h-clip_y)\n",
    "        page.set_cropbox(clip)\n",
    "        pix = page.get_pixmap()\n",
    "        pix.save(f\"{image_path}/page_{i:03d}.jpg\")\n",
    "\n",
    "    images = sorted(glob(os.path.join(image_path, \"*.jpg\")))\n",
    "\n",
    "    ### Generate image summariesd\n",
    "    print(f\"### Generating image summaries using LLM - path: {file_path}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    system_prompt = \"You are an assistant tasked with describing table or image, specialized in Smartphone product.\"\n",
    "    system_message_template = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "    human_prompt = [\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": \"data:image/png;base64,\" + \"{image_base64}\",\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": '''Given image, give a concise summary in Korean. Don't insert any XML tag such as <text> and </text> when answering.'''\n",
    "        },\n",
    "    ]\n",
    "    human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            system_message_template,\n",
    "            human_message_template\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    summarize_chain = prompt | llm | StrOutputParser()\n",
    "    base64_images = [encode_image_base64(img_path) for img_path in images]\n",
    "    image_summaries = summarize_chain.batch(base64_images, {\"max_concurrency\": 8})\n",
    "    image_summaries = remove_short_sentences(image_summaries)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Elasped {end - start:.5f} ses for generating image summaries using LLM\")\n",
    "\n",
    "    ### Generate QA pair\n",
    "    print(f\"### Generating QA pairs using LLM - path: {file_path}\")\n",
    "    start = time.time()\n",
    "\n",
    "    parser = JsonOutputParser(pydantic_object=QAPair)\n",
    "    prompt = get_qna_prompt_template()\n",
    "    #prompt = get_qna_repair_cost_prompt_template()\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    input_batch = []\n",
    "\n",
    "    for doc in image_summaries:\n",
    "        dic = {\"context\": doc, \"domain\": \"Samsung Galaxy S series Smartphone\", \"num_questions\": \"3\"}\n",
    "        input_batch.append(dic)\n",
    "\n",
    "\n",
    "    qa_pair = chain.batch(input_batch, {\"max_concurrency\": 8})\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Elasped {end - start:.5f} ses for generating image summaries using LLM\")\n",
    "\n",
    "    ### Save to jsonl for fine-tuning\n",
    "    print(f\"### Saving QA pairs to jsonl\")\n",
    "    output_dir = './dataset_tmp'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    system_prompt_msg = \"\"\"You are an AI assistant that is familiar with the details of the user manual for your Galaxy mobile phone.\n",
    "    Please answer the questions accurately. If the question is in Korean, write your answer in Korean. If the question is in English, write your answer in English.\n",
    "    \"\"\"\n",
    "\n",
    "    oai_qa_pair = convert_to_oai_format(qa_pair, system_prompt_msg=system_prompt_msg)\n",
    "\n",
    "    #save_jsonl(qa_pair, f\"{output_dir}/{save_filename}.jsonl\")\n",
    "    save_jsonl(oai_qa_pair, f\"{output_dir}/{idx}-oai.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, random\n",
    "from util.preprocess import convert_html_to_md\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "dataset_dir = \"dataset_tmp\"\n",
    "all_files = glob.glob(os.path.join(dataset_dir, \"*-oai.jsonl\"))\n",
    "\n",
    "result = []\n",
    "for f in all_files:\n",
    "    with open(f, 'r', encoding='utf-8-sig') as infile:\n",
    "        for line in infile.readlines():\n",
    "            try:\n",
    "                result.append(json.loads(line)) # read each line of the file\n",
    "            except ValueError:\n",
    "                print(f)\n",
    "\n",
    "# This would output jsonl\n",
    "with open('dataset/prod-unst-sales-brochure-oai.jsonl','w', encoding= 'utf-8-sig') as outfile:\n",
    "    for entry in result:\n",
    "        outfile.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
